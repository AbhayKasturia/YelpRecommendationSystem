{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pylab\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spark to build the model and do predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc =SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=0, product=0, rating=3.0),\n",
       " Rating(user=0, product=1, rating=3.0),\n",
       " Rating(user=0, product=2, rating=4.0),\n",
       " Rating(user=0, product=3, rating=4.0),\n",
       " Rating(user=0, product=4, rating=3.0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = sc.textFile('../Filter_Data/yelp.train.rating').map(lambda l: l.split(\"\\t\"))\n",
    "training = reviews.map(lambda l: Rating(int(l[0]),int(l[1]),float(l[2])))\n",
    "training.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for ALS\n",
    "Tuned over several runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 100\n",
    "numIter = 20\n",
    "model = ALS.train(training, rank, numIter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Predictions and calculating MSE on Training Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error On Training Data= 0.002728928954329906\n"
     ]
    }
   ],
   "source": [
    "test = training.map(lambda p: (p[0], p[1]))\n",
    "preds = model.predictAll(test).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "ratesJoinPreds = reviews.map(lambda l: ((int(l[0]),int(l[1])),int(l[2]))).join(preds)\n",
    "MSE = ratesJoinPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error On Training Data= \", MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the MSE is really low and the predictions are pretty accurate for the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Test Data\n",
    "Running Predictions and calculating MSE on Test Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 43), (1, 72), (2, 127), (3, 151), (4, 411)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_reviews = sc.textFile('../Filter_Data/yelp.test.rating').map(lambda l: l.split(\"\\t\")).map(lambda l: ((int(l[0]),int(l[1])),int(l[2])))\n",
    "testdata = test_reviews.map(lambda p: (p[0][0], p[0][1]))\n",
    "testdata.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error On Test Data = 2.2475724933205883\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "ratesAndPreds = test_reviews.join(predictions)\n",
    "MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error On Test Data = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the MSE is really bad for testing data, whereas the training data gave us really good MSE. This means that we have overfit our model!\n",
    "\n",
    "Lets look at regularizing the ALS model by tweaking the value of the regularization parameter $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "als = ALS(userCol = \"user_id\", itemCol =\"buss_id\", ratingCol= \"rating\")\n",
    "\n",
    "rank = [100]\n",
    "maxIter = [20]\n",
    "# Values of lambda choosen after several runs\n",
    "reg =  [.15,.17]\n",
    "param_grid = ParamGridBuilder().addGrid(als.rank, rank).addGrid(als.maxIter, maxIter).addGrid(als.regParam, reg).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea was to run cross validation to choose from a range of values for the parameter, but due to the memory issues on the laptop, this could not be achieved. Thus, we resorted to running several runs to pick the one with the least MSE.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(metricName= \"rmse\", labelCol= \"rating\", predictionCol= \"prediction\")\n",
    "cv = TrainValidationSplit(estimator= als, estimatorParamMaps = param_grid, evaluator= evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to DF to be used with the libraries\n",
    "training = sqlContext.createDataFrame(reviews.map(lambda x: Row(user_id=int(x[0]), buss_id=int(x[1]), rating=int(x[2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cv.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing businesses from test data which were not a part of our training dataset \n",
    "# neccessary for evaluator to work\n",
    "testdata = sqlContext.createDataFrame(test_reviews.map(lambda x: Row(user_id=x[0][0], buss_id=x[0][1], rating = x[1])))\n",
    "test_b = set(testdata.select(testdata.buss_id).distinct().rdd.map(lambda r: r[0]).collect())\n",
    "train_b = set(training.select(training.buss_id).distinct().rdd.map(lambda r: r[0]).collect())\n",
    "diff_b = test_b.difference(train_b)\n",
    "testdata = testdata.filter(~testdata.buss_id.isin(diff_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicitions and MSE on test data with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE with test data and regularization is =  1.4891042305989572\n"
     ]
    }
   ],
   "source": [
    "predictions = best_model.transform(testdata)\n",
    "evaluator = RegressionEvaluator(metricName= \"mse\", labelCol= \"rating\", predictionCol= \"prediction\")\n",
    "mse = evaluator.evaluate(predictions)\n",
    "print(\"MSE with test data and regularization is = \", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see regularization helped us bring down the overfitting and get better MSE on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hit Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Ratio is: 0.7733565788124295\n"
     ]
    }
   ],
   "source": [
    "threshold = 1.5\n",
    "print(\"Hit Ratio is:\", predictions.filter((predictions.rating < predictions.prediction+threshold) & (predictions.rating > predictions.prediction-threshold)).count()/predictions.count()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the threshold of +-1.5 we get the above hit ratio, which is descent but not perfect. In order to improve it ffurther we need to increase the number of ranks to be involved in model building. We had to restrict ourselves to a max of 100 because of resource constraints(memory and cpu), but when utilizing a cloud based platform, it should give us much better hit ratio and MSE for higher number of ranks. More in the report."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
